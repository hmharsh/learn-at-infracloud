kube_endpoint_address_available



# prometheus has its own dashboard called expression browser
# prometheusQL is the language used to query data in prometheus
# prometheus ask cAdvisor in kubernetes for all metrices 

# Every time series is uniquely identified by its metric name and optional key-value pairs called labels.
    <metric name>{<label name>=<label value>, ...}
# Prometheus installation on kubernetes cluster
 use (remember to change hostname of kubernetes cluster nodes from config map) -> https://github.com/linuxacademy/content-kubernetes-prometheus-env, explained here https://linuxacademy.com/blog/kubernetes/running-prometheus-on-kubernetes/, Helm can also be use for istallation 
 Also there are deployment and service to deploy graphana (change password from deployment file 1st)


# The Prometheus client libraries offer four core metric types
Counter, Gauge, Histogram, Summary
# there are two ways to configure prometheus
 1. set command line flags (http://13.232.126.149/flags)
 2. config file (http://13.232.126.149/config)


Node exporter

```
 adduser prometheus
 cd /home/prometheus
 curl -LO "https://github.com/prometheus/node_exporter/releases/download/v0.16.0/node_exporter-0.16.0.linux-amd64.tar.gz"
 tar -xvzf node_exporter-0.16.0.linux-amd64.tar.gz
 mv node_exporter-0.16.0.linux-amd64 node_exporter
 cd node_exporter
 chown prometheus:prometheus node_exporter


cat <<EOT >> /etc/systemd/system/node_exporter.service
    [Unit]
    Description=Node Exporter
    
    [Service]
    User=prometheus
    ExecStart=/home/prometheus/node_exporter/node_exporter
    
    [Install]
    WantedBy=default.target
EOT
systemctl daemon-reload
systemctl enable --now node_exporter.service
systemctl status node_exporter.service
```
Now check updates in prometheus


# Implementing with node.js
refer: https://github.com/linuxacademy/content-kubernetes-prometheus-app
-> create api end point in app for timeseries data, in node.js swagger-states 3rd party is a module that gather all applicatrion related data (that comes out on console) and show it on one uri in timeseries formate, also provide a nice UI based dashboard
-> lets say URI of endpoint is localhost:3000/swagger-stats/metrics
-> create dockerfile like 
  ```
    FROM node:alpine
    RUN mkdir /var/node
    WORKDIR /var/node
    ADD . /var/node
    RUN npm install
    EXPOSE 3000
    CMD npm start
  ```
->  push image to dockerhub
-> now in kubernetes (point the annotations)
              prometheus.io/scrape: "true"
              prometheus.io/path: 'swagger-stats/metrics'
              prometheus.io/port: "3000"

```
  apiVersion: v1
  kind: Service
  metadata:
    name: comicbox-service
  spec:
    selector:
      app: comicbox
    type: NodePort
    ports:
      - port: 3000
        targetPort: 3000
        nodePort: 8001
  ---
  apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    name: comicbox
  spec:
    replicas: 1
    template:
      metadata:
        labels:
          app: comicbox
        annotations:
              prometheus.io/scrape: "true"
              prometheus.io/path: 'swagger-stats/metrics'
              prometheus.io/port: "3000"
      spec:
        containers:
          - name: comicbox
            image: rivethead42/comicbox
            ports:
              - containerPort: 3000
```


-> now go to grafana, import swagger-stats (3091) type dashboard on prometheus data and that's, data is live

# PomQL
4 data types

Instant vector - a set of time series containing a single sample for each time series, all sharing the same timestamp
Range vector - a set of time series containing a range of data points over time for each time series
Scalar - a simple numeric floating point value
String - a simple string value; currently unused

simple filters
kube_node_status_allocatable_cpu_cores {job="kubernetes-service-endpoints", node="arpitsinghal5c.mylabserver.com" }

kube_node_status_allocatable_cpu_cores {job="kubernetes-service-endpoints"}[5m]
m is range vector selector like (s.m.h,d,w,y)
The rate() function in Prometheus looks at the history of time series over a time period, and calculates how fast it's increasing per second.
irate() is the per second rate of change at the end of your range interval. (other is ignored just last two are used)

=: Select labels that are exactly equal to the provided string.
!=: Select labels that are not equal to the provided string.
=~: Select labels that regex-match the provided string.
!~: Select labels that do not regex-match the provided string.

(~ means begain of regex) as below
Query job that end with -exporter: -> node_cpu_seconds_total{job=~".*-exporter"}
Query job that begins with kube: -> container_cpu_load_average_10s{job=~"^kube.*"}


PomQL cheatsheet: https://timber.io/blog/promql-for-humans
Examples: 
  {__name__="go_gc_duration_seconds",}
  ({__name__=~"[a-z A-Z].*"})
  node_network_receive_bytes_total{device=~"eth1|lo"}
  rate(node_cpu_seconds_total {cpu = "0"}[5m]) *2   (operators with scalar*scalar, scalar*vactort, vactor*vactor)
  vector1 and vector2 results in a vector consisting of the elements of vector1 for which there are elements in vector2
  http_requests_total offset 5m  -> The offset modifier allows changing the time offset for individual instant and range vectors in a query.
  sum(http_requests_total{method="GET"} offset 5m) // GOOD.
  sum(http_requests_total{method="GET"}) offset 5m // INVALID.
  <aggr-op> [without|by (<label list>)] ([parameter,] <vector expression>) or <aggr-op>([parameter,] <vector expression>) [without|by (<label list>)]
  sum without (instance) (http_requests_total)
  sum by (application, group) (http_requests_total)
  sum by (instance) (node_filesystem_avail_bytes {fstype !="squashfs"} )
  count_values("version", build_version) or count_values("x", http_request_duration_microseconds)  ==> count the similar values (right coloum) in specific vector (eg http_request_duration_microseconds) and display that count value with name as 1st perameter (eg 'x')
  topk(6, http_requests_total)
  absent(some vactor return is dosent return anything) => return value 1
  absent_over_time ( http_request_duration_microseconds {instance="1244.2.87:9090"}[1h])
  changes (returns the number of times its value has changed within the provided time range as an instant vector)
  delta( http_request_duration_microseconds [5m] ) -> difference in first and last value of each time series similarly to 'idelta' which calculate difference b/w last two
  deriv( http_request_duration_microseconds [5m] )  -> deriavative of timeseries (regression)
  label_join(http_request_duration_microseconds, "harshit", "-", "instance", "quantile", "kubernetes_io_hostname"  ) // will insert a new label in data named harshit which is equal to concatnation od values "instance", "quantile", "kubernetes_io_hostname" from actual output  saparated by delimiter ("-")
  label_replace(label_join(http_request_duration_microseconds, "harshit", "-", "instance", "quantile", "kubernetes_io_hostname"  ) , "mahajan", "$1", "harshit", "(.*):.*" )
  sort(http_requests_total)
  sort_desc(http_requests_total)
  sqrt(http_requests_total)
  time()
  vector(time()) // return scalar as vector with no label 
  <aggregation>_over_time() , apply aggregation function on time => eg: count_over_time(http_request_duration_microseconds[5m])
  subquery: rate(http_requests_total[5m])[30m:1m] => Return the 5-minute rate of the http_requests_total metric for the past 30 minutes, with a resolution of 1 minute.
  avg by (code) (http_requests_total)
  sum(http_requests_total) by (instance)
  sum by  (instance,quantile) (http_request_duration_microseconds)
  sum by (instance) (up {kubernetes_namespace="monitoring"})
  metric1 or metric2 or metric3 => (http_request_duration_microseconds or node_memory_Active_bytes)

  HTTP API query (with get/post both),  "resultType": "matrix" | "vector" | "scalar" | "string"
  URL query parameters:

   query=<string>: Prometheus expression query string.
   time=<rfc3339 | unix_timestamp>: Evaluation timestamp. Optional.
   timeout=<duration>: Evaluation timeout. Optional. Defaults to and is capped by the value of the -query.timeout flag.

  http://13.233.147.158/api/v1/query?query=http_requests_total
  http://13.233.147.158/api/v1/query?query=sum(http_requests_total)
  http://13.233.147.158/api/v1/query?query=http_requests_total[5m]
  http://13.233.147.158/api/v1/query?query=up{app="httpd"}
  http://13.233.147.158/api/v1/query?query=up&time=2020-04-01T20:10:51.781Z --> get result for specific time
  http://13.233.147.158/api/v1/labels
  http://13.233.147.158/api/v1/targets and  http://13.233.147.158/api/v1/targets/metadata
  http://13.233.147.158/api/v1/targets?state=active
  http://13.233.147.158/api/v1/rules
  http://13.233.147.158/api/v1/alerts
  http://13.233.147.158/api/v1/metadata
  http://13.233.147.158/api/v1/metadata?metric=http_requests_total
  http://13.233.147.158/api/v1/alertmanagers
  http://13.233.147.158/api/v1/status/config => configuration file
  http://13.233.147.158/api/v1/status/flags
  http://13.233.147.158/api/v1/status/runtimeinfo
  http://13.233.147.158/api/v1/status/buildinfo
  http://13.233.147.158/api/v1/status/tsdb // time series database
  

  Management API
   http://13.233.147.158/-/ready  // rediness check
   http://13.233.147.158/-/reload
   http://13.233.147.158/-/healthy
   http://13.233.147.158/-/quit
   


  once admin api is enabled by setting --web.enable-admin-api, tsdb snapshot can also be generated by api 
  POST /api/v1/admin/tsdb/snapshot or
  PUT /api/v1/admin/tsdb/snapshot
  // both of this return dabase filename

  deletion of data from timeseries database:  /api/v1/admin/tsdb/delete_series  with conditions specified in parameters
  clean tombstones:  POST /api/v1/admin/tsdb/clean_tombstones or PUT /api/v1/admin/tsdb/clean_tombstones





  






  curl 'http://localhost:9090/api/v1/query_range?query=up&start=2015-07-01T20:10:30.781Z&end=2015-07-01T20:11:00.781Z&step=15s'
  curl -g 'http://localhost:9090/api/v1/series?' --data-urlencode 'match[]=up' --data-urlencode 'match[]=process_start_time_seconds{job="prometheus"}'




To understand: 
  holt_winters
  histogram_quantile
  exp
  deriv
  clamp_max and clamp_min
  increase
  irate and rate
  


--> (query operators) https://prometheus.io/docs/prometheus/latest/querying/operators/ 
-> group by 'instance': 
avg(irate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[5m])) by (instance)

# Rules
-> just yml files where like
```
node_rules.yml: |-
 groups:
 - name: node_rules
   interval: 10s
   rules:
     - record: instance:node_cpu:avg_rate5m
       expr: 100 - avg(irate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[5m])) by (instance) * 100
     - record: instance:node_memory_usage:percentage
       expr: ((sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes) - sum(node_memory_Buffers_bytes) - sum(node_memory_Cached_bytes)) / sum(node_memory_MemTotal_bytes)) * 100
     - record: instance:root:node_filesystem_usage:percentage
       expr: (node_filesystem_size_bytes{mountpoint="/rootfs"} - node_filesystem_free_bytes{mountpoint="/rootfs"}) /node_filesystem_size_bytes{mountpoint="/rootfs"} * 100
```

Where record is just a unique name and expr is PomQL expression, which keep on recoading in time series formate and we can check this values anytime

Just put this file (refer: https://github.com/linuxacademy/content-kubernetes-prometheus-env/tree/master/readrules)
-> edit deployment (to mount rule directory), configmap (to add path of rules), and create a new cm with rule file, and apply changes to update running cluster

https://hooks.slack.com/services/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


# Alerting rule example:

groups:
- name: example
  rules:
  - alert: HighRequestLatency
    expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5  # PomQL
    for: 10m # look for 10 min after catching and if not resolved, send the alert
    labels:
      severity: page
    annotations:
      summary: High request latency

 multiple groups can be there in a file as: https://github.com/linuxacademy/content-kubernetes-prometheus-env/blob/master/alertmanager/prometheus-rules-config-map.yml



 alert to ensure availability of some specific container

      
      - alert: RedisServerDown
        expr: redis_up{app="media-redis"} == 0  # scrap(to true) and port in annotation must be set for prometheus to fetch data 
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: Redis Server {{ $labels.instance }} is down!
      

Most useful blog for getting relabel config: https://medium.com/quiq-blog/prometheus-relabeling-tricks-6ae62c56cbda

Then, reload the Prometheus configuration:
$ curl -X POST http://127.0.0.1:9090/-/reload

- Recording and alerting rules exist in a rule group. Rules within a group are run sequentially at a regular interval.

- promtool to check stntax written in yaml before applying those rule 
  $ go get github.com/prometheus/prometheus/cmd/promtool
  $ promtool check rules /path/to/example.rules.yml



# Prometheus with node.js app
https://github.com/linuxacademy/content-kubernetes-prometheus-app
